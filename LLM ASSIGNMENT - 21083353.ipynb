{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fb9caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (0.11.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from transformers[torch]) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->transformers[torch]) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in c:\\users\\ad22acb\\appdata\\roaming\\python\\python310\\site-packages (0.29.3)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from accelerate) (1.12.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from accelerate) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ad22acb\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda2023.03-1\\lib\\site-packages (from tqdm->huggingface-hub->accelerate) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#Installing the required libraries for working with transformers and model acceleration\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af043b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries and frameworks\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13525937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting logging level to ERROR for transformers to avoid clutter\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f244930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe to store literature review summaries\n",
    "columns = ['Paper', 'Problem Addressed', 'Methods Used', 'Key Findings', 'URL']\n",
    "literature_review = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d3b4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Paper  \\\n",
      "0  Exploring the Limits of Transfer Learning with...   \n",
      "\n",
      "                                   Problem Addressed  \\\n",
      "0  Investigate the capabilities and limitations o...   \n",
      "\n",
      "                                        Methods Used  \\\n",
      "0  Uses the T5 model which treats every NLP probl...   \n",
      "\n",
      "                                        Key Findings  \\\n",
      "0  Shows that a single model can perform well acr...   \n",
      "\n",
      "                                URL  \n",
      "0  https://arxiv.org/abs/1910.10683  \n"
     ]
    }
   ],
   "source": [
    "#Function to add summaries of papers to the dataframe\n",
    "def add_paper_summary(paper, problem, methods, findings, url):\n",
    "    global literature_review\n",
    "    summary = pd.DataFrame([[paper, problem, methods, findings, url]], columns=columns)\n",
    "    literature_review = pd.concat([literature_review, summary], ignore_index=True)\n",
    "    \n",
    "#Adding a sample paper summary to the literature review dataframe\n",
    "add_paper_summary(\n",
    "    paper=\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n",
    "    problem=\"Investigate the capabilities and limitations of transfer learning in NLP using a unified model architecture.\",\n",
    "    methods=\"Uses the T5 model which treats every NLP problem as a text-to-text task.\",\n",
    "    findings=\"Shows that a single model can perform well across diverse NLP tasks, suggesting the efficacy of transfer learning.\",\n",
    "    url=\"https://arxiv.org/abs/1910.10683\"\n",
    ")\n",
    "\n",
    "#Printing the dataframe to verify contents\n",
    "print(literature_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0761644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3551,  0.0686,  0.6506],\n",
      "         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],\n",
      "         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],\n",
      "         ...,\n",
      "         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],\n",
      "         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],\n",
      "         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]])\n"
     ]
    }
   ],
   "source": [
    "#Initializing a tokenizer and a model for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#Setting the model to evaluation mode\n",
    "model.eval()  \n",
    "\n",
    "#Sample text for encoding\n",
    "text = \"Here is some text to encode\"\n",
    "\n",
    "#Encoding the text using the BERT tokenizer\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "#Performing a forward pass to get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "#Extracting the last hidden states from the model output\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e77053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring and initializing a BERT Model for Masked Language Modeling (MLM)\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4330b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2023.03-1\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 109514298\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in fine-tuned-bert\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 71.8992, 'train_samples_per_second': 0.042, 'train_steps_per_second': 0.042, 'train_loss': 6.790072123209636, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in fine-tuned-bert\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#Defining a custom dataset class for handling text data for language modeling\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "\n",
    "#Setting up tokenizer and data collator for masked language modeling\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "#Creating a dataset and setting up training arguments\n",
    "train_texts = [\"Insert your training texts here...\"]\n",
    "train_dataset = MyDataset(train_texts, tokenizer, max_length=128)\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "#Initializing the Trainer and training the model, and later saving it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"fine-tuned-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f0cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['runname', 'steps', 'agg_score', 'commonsense_qa/acc',\n",
      "       'commonsense_qa/acc_norm', 'hellaswag/acc', 'hellaswag/acc_norm',\n",
      "       'openbookqa/acc', 'openbookqa/acc_norm', 'piqa/acc', 'piqa/acc_norm',\n",
      "       'siqa/acc', 'siqa/acc_norm', 'winogrande/acc', 'winogrande/acc_norm',\n",
      "       'sciq/acc', 'sciq/acc_norm', 'arc/acc', 'arc/acc_norm', 'mmlu/acc',\n",
      "       'mmlu/acc_norm'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Reading the csv file of the dataset and storing it in a variable named df\n",
    "df = pd.read_csv('eval_results.csv')\n",
    "\n",
    "#Printing the columns of the dataset\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205f5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the name of the column in the dataframe that contains the input text for the model\n",
    "input_column = 'runname'\n",
    "\n",
    "#Defining the name of the column in the dataframe that contains the target values or labels for the model\n",
    "label_column = 'agg_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556abce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing datasets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f22b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\ad22acb/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ad22acb/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\ad22acb/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\ad22acb/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ad22acb/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.14177370071411133\n"
     ]
    }
   ],
   "source": [
    "#Defining another custom dataset class for text classification\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.iloc[index][input_column])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, None, add_special_tokens=True, max_length=self.max_len,\n",
    "            padding='max_length', return_token_type_ids=True, truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        return {'ids': torch.tensor(ids, dtype=torch.long), 'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.data.iloc[index][label_column], dtype=torch.float)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "#Initializing data loaders for the training and testing sets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "training_set = TextDataset(train_df, tokenizer)\n",
    "testing_set = TextDataset(test_df, tokenizer)\n",
    "\n",
    "#Setting parameters for DataLoader\n",
    "train_params = {'batch_size': 16, 'shuffle': True}\n",
    "test_params = {'batch_size': 16, 'shuffle': True}\n",
    "\n",
    "#Creating DataLoader for training and testing\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "#Setting up and training a BERT model for sequence classification\n",
    "num_labels = 1\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "#Training and validating the sequence classification model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device)\n",
    "        mask = data['mask'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "        outputs = model(ids, mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(outputs.logits.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if _ % 500 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "\n",
    "for epoch in range(3):\n",
    "    train(epoch)\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device)\n",
    "            mask = data['mask'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "            outputs = model(ids, mask)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy())\n",
    "            fin_outputs.extend(outputs.logits.squeeze().cpu().detach().numpy())\n",
    "    return fin_targets, fin_outputs\n",
    "\n",
    "#Calling validate function to get predictions and true values from the test set\n",
    "targets, outputs = validate()\n",
    "\n",
    "#Calculating and printing the mean squared error between predicted outputs and actual targets\n",
    "val_loss = torch.nn.functional.mse_loss(torch.tensor(outputs), torch.tensor(targets))\n",
    "print(\"Validation Loss:\", val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e2b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
